# ml-audits

This repository hosts the code for our work on multi-agent collaborative auditing.

# Setting up the python environment

Create a new conda environment using the provided `environment.yml` file.

```bash
conda env create -f environment.yml
```

Activate the environment before running any code below.

```bash
conda activate audits
```

# Setting up the datasets

## German Credit

```python
# Run from root directory
python scripts/preprocess_german_credit.py
```

The script downloads the dataset, preprocesses it and saves it in the `data/german_credit` folder. The preprocessed dataset is saved as `data/german_credit/features.csv` and `data/german_credit/labels.csv`.

## Propublica

Manually create the folder `data/propublica`, then download `compas-scores-two-years.csv` from [ProPublica GitHub](https://github.com/propublica/compas-analysis/tree/master) and save it in the `data/propublica` folder. Finally run the following script to complete the preprocessing.

```python
# Run from root directory
python scripts/preprocess_propublica_dataset.py
```

You should now have the preprocessed dataset in the `data/propublica` folder as `data/propublica/features.csv` and `data/propublica/labels.csv`.

## Folktables

Manually create the folder `data/folktables` before running the following. We will first download the dataset using the following script. Note that this download takes a while as the dataset is large. Additionally you will need a machine with a large amount of memory (150G) to run the download script.

```python
# Run from root directory
python scripts/download_folk_tables.py
```

The raw data gets downloaded in the `data/2018/5-year` folder which is processed and saved as `data/folktables/features.csv` and `data/folktables/labels.csv`. We will now binarize the features using the following script.

```python
# Run from root directory
python scripts/preprocess_folk_tables.py
```

After running the script, you should have the final preprocessed dataset as `data/folktables/features_bin.csv` and `data/folktables/labels_bin.csv`.

## Confirm download of datasets

We require additional information about the datasets to run our main experiments.
This information includes strata sizes, ground truth demographic parity, etc.
While we can generate this information within each run of the main experiment, it is more efficient to generate this information once and save it for future use.
Run `analyze_dataset.py` which will do all the above things and generate meta files `data/<dataset_name>/all_probs.pkl`, `data/<dataset_name>/all_ys.pkl`and `data/<dataset_name>/all_nks.pkl` for each dataset.

```python
# Run from root directory
python analyze_dataset.py
```

This script also prints P(X_i = 1) for each attribute in the dataset and furthermore the ground truth demographic parity i.e. P(Y = 1 | X_i = 1) - P(Y = 1 | X_i = 0) for each attribute. Tables 2, 3 and 4 in the Appendix of the paper present some of this information generated by `analyze_dataset.py`.

# Running two agent collaboration (Figure 3)

To run the two agent collaboration experiment, run the following script:

```bash
scripts/two_agent.sh
```

You can modify the script to change the `dataset`. Please set the number of `repetitions` depending upon the chosen dataset as indicated in the script. These repetitions are chosen considering the size of the dataset (i.e. corresponding run time) as well as the accuracy of estimation. You must also set the `attrs_to_audit` variable depending upon the dataset as indicated in the script.

# Steps-to-run

1. Ensure that you have dataset files in the `data` folder. These should go as `data/<dataset_name>/features.csv` and `data/<dataset_name>/labels.csv` where `<dataset_name>` could be one of `folktables`, `german_credit`, `propublica` or `synthetic`. 
2. Run `analyze_dataset.py` by updating the `DATASETS = [...]` list to include the dataset names you want to analyze. This step is important as it will generate the required probabilities for debiasing, saved as `data/<dataset_name>/all_probs.pkl` and `data/<dataset_name>/all_ys.pkl`.
3. To generate gain plots, you will have to launch `run_multi_colab.py` as follows:

```
python run_multi_colab.py --repetitions 500 --dataset <dataset_name>  --seed 112 --oversample --sample stratified --collaboration <collab_type> --budget 200
```

where `<collab_type>` could be one of `none`, `aposteriori` or `apriori`. The result files will be generated in `results/<dataset_name>/multicolab_b<budget>`.