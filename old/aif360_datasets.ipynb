{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:No module named 'tempeh': fetch_lawschool_gpa will be unavailable. To install, run:\n",
      "pip install 'aif360[LawSchoolGPA]'\n"
     ]
    }
   ],
   "source": [
    "from aif360.sklearn.datasets import fetch_german"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import mean\n",
    "from numpy import std\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder, MinMaxScaler\n",
    "from sklearn.metrics import fbeta_score, make_scorer\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold, cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from matplotlib import pyplot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset\n",
    "def load_dataset():\n",
    "    # load the dataset as a numpy array\n",
    "    dataset = fetch_german()\n",
    "    # split into inputs and outputs\n",
    "    X, y = dataset.X, dataset.y\n",
    "    # select categorical features\n",
    "    cat_ix = X.select_dtypes(include=['category']).columns\n",
    "    num_ix = X.select_dtypes(include=['int64', 'float64']).columns\n",
    "    # one hot encode cat features only\n",
    "    # label encode the target variable to have the classes 0 and 1\n",
    "    y = LabelEncoder().fit_transform(y)\n",
    "    return X, y, cat_ix, num_ix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dhasade/anaconda3/envs/audits/lib/python3.8/site-packages/sklearn/datasets/_openml.py:1002: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "X, y, cat_ix, num_ix = load_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate f2 score\n",
    "def f2(y_true, y_pred):\n",
    "\treturn fbeta_score(y_true, y_pred, beta=2)\n",
    "\n",
    "# evaluate a model\n",
    "def evaluate_model(X, y, model):\n",
    "\t# define evaluation procedure\n",
    "\tcv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "\t# define the model evaluation the metric\n",
    "\tmetric = make_scorer(f2)\n",
    "\t# evaluate model\n",
    "\tscores = cross_val_score(model, X, y, scoring=metric, cv=cv, n_jobs=-1)\n",
    "\treturn scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the reference model\n",
    "model = DummyClassifier(strategy='constant', constant=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean F2: 0.921 (0.000)\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model\n",
    "scores = evaluate_model(X, y, model)\n",
    "# summarize performance\n",
    "print('Mean F2: %.3f (%.3f)' % (mean(scores), std(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define models to test\n",
    "def get_models():\n",
    "    models, names = list(), list()\n",
    "    # LR\n",
    "    models.append(LogisticRegression(solver='liblinear'))\n",
    "    names.append('LR')\n",
    "    # LDA\n",
    "    models.append(LinearDiscriminantAnalysis())\n",
    "    names.append('LDA')\n",
    "    # NB\n",
    "    models.append(GaussianNB())\n",
    "    names.append('NB')\n",
    "    # GPC\n",
    "    models.append(GaussianProcessClassifier())\n",
    "    names.append('GPC')\n",
    "    # SVM\n",
    "    models.append(SVC(gamma='scale'))\n",
    "    names.append('SVM')\n",
    "    return models, names\n",
    "\n",
    "models, names = get_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">LR 0.862 (0.033)\n",
      ">LDA 0.855 (0.032)\n",
      ">NB 0.682 (0.148)\n",
      ">GPC 0.910 (0.016)\n",
      ">SVM 0.887 (0.025)\n"
     ]
    }
   ],
   "source": [
    "results = list()\n",
    "# evaluate each model\n",
    "for i in range(len(models)):\n",
    " # one hot encode categorical, normalize numerical\n",
    " ct = ColumnTransformer([('c',OneHotEncoder(),cat_ix), ('n',MinMaxScaler(),num_ix)])\n",
    " # wrap the model i a pipeline\n",
    " pipeline = Pipeline(steps=[('t',ct),('m',models[i])])\n",
    " # evaluate the model and store results\n",
    " scores = evaluate_model(X, y, pipeline)\n",
    " results.append(scores)\n",
    " # summarize and store\n",
    " print('>%s %.3f (%.3f)' % (names[i], mean(scores), std(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct = ColumnTransformer([('c',OneHotEncoder(),cat_ix), ('n',MinMaxScaler(),num_ix)])\n",
    "X = ct.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_count = 0\n",
    "for x in ct.named_transformers_['c'].categories_:\n",
    "    total_count += len(x)\n",
    "    print(len(x))\n",
    "print(total_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dhasade/anaconda3/envs/audits/lib/python3.8/site-packages/sklearn/datasets/_openml.py:1002: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "X, y, cat_ix, num_ix = load_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform the age column into zero and one depending on the age being greater than 25\n",
    "X['age'] = X['age'].apply(lambda x: 0 if x <= 25 else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform the sex column into 0 or 1\n",
    "X['sex'] = X['sex'].apply(lambda x: 0 if x == 'female' else 1).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(age = 1): 0.81\n",
      "P(age = 0): 0.18999999999999995\n",
      "P(sex = 1): 0.69\n",
      "P(sex = 0): 0.31000000000000005\n"
     ]
    }
   ],
   "source": [
    "# Calculate the probabilities\n",
    "prob_age_1 = X['age'].mean()  # P(age = 1)\n",
    "prob_age_0 = 1 - prob_age_1   # P(age = 0)\n",
    "\n",
    "prob_sex_1 = X['sex'].mean()  # P(sex = 1)\n",
    "prob_sex_0 = 1 - prob_sex_1   # P(sex = 0)\n",
    "\n",
    "print(\"P(age = 1):\", prob_age_1)\n",
    "print(\"P(age = 0):\", prob_age_0)\n",
    "print(\"P(sex = 1):\", prob_sex_1)\n",
    "print(\"P(sex = 0):\", prob_sex_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(y=1|age=0): 0.5789473684210527\n",
      "P(y=1|age=1): 0.7283950617283951\n",
      "P(y=1|sex=0): 0.6483870967741936\n",
      "P(y=1|sex=1): 0.7231884057971014\n"
     ]
    }
   ],
   "source": [
    "# Calculate the conditional probabilities\n",
    "prob_y_given_age_0 = y[X['age'] == 0].mean()  # P(y=1|age=0)\n",
    "prob_y_given_age_1 = y[X['age'] == 1].mean()  # P(y=1|age=1)\n",
    "\n",
    "prob_y_given_sex_0 = y[X['sex'] == 0].mean()  # P(y=1|sex=0)\n",
    "prob_y_given_sex_1 = y[X['sex'] == 1].mean()  # P(y=1|sex=1)\n",
    "\n",
    "print(\"P(y=1|age=0):\", prob_y_given_age_0)\n",
    "print(\"P(y=1|age=1):\", prob_y_given_age_1)\n",
    "print(\"P(y=1|sex=0):\", prob_y_given_sex_0)\n",
    "print(\"P(y=1|sex=1):\", prob_y_given_sex_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "audits",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
